{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47d3205",
   "metadata": {},
   "source": [
    "## Web Scraping Demo\n",
    "This is a simple demonstration of web scraping using Python. The code uses libraries such as `requests` and `BeautifulSoup` to extract job data from websites like Wuzzuf and Bayt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effd65f",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Libraries\n",
    "Install the `lxml` library for HTML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ce27d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp313-cp313-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 588.1 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 588.1 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.8/3.8 MB 752.6 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 777.2 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 1.3/3.8 MB 845.3 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.3/3.8 MB 845.3 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 848.2 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 848.2 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.8/3.8 MB 822.0 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.1/3.8 MB 817.6 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 2.4/3.8 MB 846.1 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.8 MB 846.1 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.9/3.8 MB 914.1 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.9/3.8 MB 914.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 876.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 876.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 864.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 902.5 kB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c975f",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "Import the necessary libraries for making HTTP requests and parsing HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453dad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d988e9",
   "metadata": {},
   "source": [
    "### Step 3: Scrape Wuzzuf for Job Data\n",
    "Define the target URL and fetch the page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd6b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for Wuzzuf search page\n",
    "u = \"https://wuzzuf.net/search/jobs?a=spbg&q=machine%20learning\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "page = requests.get(u)\n",
    "print(page)\n",
    "\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907519d",
   "metadata": {},
   "source": [
    "#### Step 3.1: Extract Job Titles\n",
    "Use BeautifulSoup to find and print all job titles on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c679d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Lead\n",
      "AI Engineer\n",
      "AI/ML Python Developer\n",
      "Data Analyst\n",
      "AI Engineer\n",
      "Robotics and Programming Engineer\n",
      "Sales Specialist\n",
      "AWS Cloud Administrator\n",
      "Senior Full Stack PHP Laravel Developer (Remote - Full Time)\n",
      "AI Specialist\n",
      "GIS Technician\n",
      "CAM Engineer\n",
      "Senior IT Help Desk\n",
      "Receptionist\n",
      "Chief Technology Officer / Co-Founder\n"
     ]
    }
   ],
   "source": [
    "# Find all job titles on the page\n",
    "job_titles = soup.find_all(\"h2\", class_=\"css-m604qf\")\n",
    "for job_title in job_titles:\n",
    "    print(job_title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6996fb4",
   "metadata": {},
   "source": [
    "#### Step 3.2: Extract Job Locations\n",
    "Extract and print the locations of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b37742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maadi, Cairo, Egypt \n",
      "Cairo, Egypt \n",
      "Riyadh, Saudi Arabia \n",
      "Azarita, Alexandria, Egypt \n",
      "Heliopolis, Cairo, Egypt \n",
      "Alexandria, Egypt \n",
      "Downtown, Cairo, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "Cairo, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "Nasr City, Cairo, Egypt \n",
      "New Nozha, Cairo, Egypt \n",
      "Madinaty, Cairo, Egypt \n",
      "New Cairo, Cairo, Egypt \n"
     ]
    }
   ],
   "source": [
    "# Find all job locations\n",
    "job_locations = soup.find_all(\"span\", class_=\"css-5wys0k\")\n",
    "for job_location in job_locations:\n",
    "    print(job_location.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a0613",
   "metadata": {},
   "source": [
    "#### Step 3.3: Extract Company Names\n",
    "Extract and print the company names associated with the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e03db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WUZZUF -\n",
      "Rehabitaire -\n",
      "BrainBox -\n",
      "Mobility Pro DMCC -\n",
      "Integrated Technology Group -\n",
      "Smart Technology -\n",
      "Gila Electric -\n",
      "Citylogix ME -\n",
      "HIGHBASE TRADING W.L.L -\n",
      "LINK Development -\n",
      "Citylogix ME -\n",
      "Mekano -\n",
      "Early Arrive   -\n",
      "Body Fit EMS fitness  -\n",
      "Confidential -\n"
     ]
    }
   ],
   "source": [
    "# Find all company names\n",
    "company_names = soup.find_all(\"a\", class_=\"css-17s97q8\")\n",
    "for company_name in company_names:\n",
    "    print(company_name.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b522d4",
   "metadata": {},
   "source": [
    "#### Step 3.4: Extract Job Post Dates\n",
    "Extract and print when the jobs were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "858e39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 days ago\n",
      "15 days ago\n",
      "29 days ago\n",
      "1 month ago\n",
      "1 month ago\n",
      "2 months ago\n",
      "1 month ago\n",
      "27 days ago\n",
      "17 days ago\n",
      "25 days ago\n",
      "5 days ago\n",
      "20 days ago\n",
      "3 days ago\n",
      "12 days ago\n",
      "17 days ago\n"
     ]
    }
   ],
   "source": [
    "# Find all job posting dates\n",
    "job_dates= soup.find_all(\"div\", class_= [\"css-4c4ojb\", \"css-do6t5g\"])\n",
    "for job_date in job_dates:\n",
    "    print(job_date.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b3733",
   "metadata": {},
   "source": [
    "#### Step 3.5: Extract Job Types\n",
    "Extract and print the types of jobs (e.g., Full-time, Part-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf1f44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n",
      "Full Time\n"
     ]
    }
   ],
   "source": [
    "# Find all job types\n",
    "job_types = soup.find_all(\"span\", class_=\"css-1ve4b75 eoyjyou0\")\n",
    "for job_type in job_types:\n",
    "    print(job_type.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73568e",
   "metadata": {},
   "source": [
    "#### Step 3.6: Extract Job URLs\n",
    "Extract and print the URLs for individual job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86836d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://wuzzuf.net/jobs/p/4Nf60vomcxgU-Machine-Learning-Lead-WUZZUF-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/Xu4wzCQcUFb8-AI-Engineer-Rehabitaire-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/yOPi6zGlGA0e-AIML-Python-Developer-BrainBox-Riyadh-Saudi-Arabia', 'https://wuzzuf.net/jobs/p/MSKFu1ytuKG0-Data-Analyst-Mobility-Pro-DMCC-Alexandria-Egypt', 'https://wuzzuf.net/jobs/p/mJweLehwNSbz-AI-Engineer-Integrated-Technology-Group-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/jjEycNgftu1U-Robotics-and-Programming-Engineer-Smart-Technology-Alexandria-Egypt', 'https://wuzzuf.net/jobs/p/PfXPkSfnCCNs-Sales-Specialist-Gila-Electric-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/XgaTEqFbNrDY-AWS-Cloud-Administrator-Citylogix-ME-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/mXk2cwxACW2l-Senior-Full-Stack-PHP-Laravel-Developer-Remote---Full-Time-HIGHBASE-TRADING-W-L-L-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/3rg82U4xRJsY-AI-Specialist-LINK-Development-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/vm7kDATBesFt-GIS-Technician-Citylogix-ME-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/eS0W2YKv55kz-CAM-Engineer-Mekano-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/gTubtDHK7cP8-Senior-IT-Help-Desk-Early-Arrive-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/SUpTnZfvqacB-Receptionist-Body-Fit-EMS-fitness-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/tweBrY9aCGPU-Chief-Technology-Officer-Co-Founder-Cairo-Egypt']\n"
     ]
    }
   ],
   "source": [
    "# Extract job URLs\n",
    "# there was an issue: we went to the higher class in the html content to avoid this problem\n",
    "job_urls=[]\n",
    "urls= soup.find_all(\"h2\", class_=\"css-m604qf\")\n",
    "for job_url in urls:\n",
    "    job_urls.append(job_url.find(\"a\").attrs[\"href\"])\n",
    "print(job_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3d2b7",
   "metadata": {},
   "source": [
    "#### Step 3.7: Fetch Individual Job Details\n",
    "Fetch and print the titles of individual jobs from their URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfede130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Example URL for testing\n",
    "url = \"https://wuzzuf.net/jobs/p/5NFxSMKMH5K0-SeniorMid-Senior-Deep-Learning-Engineer-Cairo-Egypt?o=1&l=sp&t=sj&a=machine%20learning|search-v3|spbg\"\n",
    "\n",
    "# Send a GET request and parse the content\n",
    "page = requests.get(url)\n",
    "print(page)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b251a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Management Jobs\n",
      "Management Jobs\n",
      "Experienced Jobs\n",
      "Entry Level Jobs\n",
      "Internships\n",
      "All Jobs\n"
     ]
    }
   ],
   "source": [
    "jobs_titles = soup.find_all(\"span\", class_=\"css-w7fd6s\")\n",
    "for job_title in jobs_titles:\n",
    "    print(job_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba421ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee778eef",
   "metadata": {},
   "source": [
    "### Step 4: Scrape Bayt for Job Data\n",
    "Switch to a different platform and extract job data from Bayt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee1f0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Define the Bayt URL for data science jobs\n",
    "url = \"https://www.bayt.com/en/egypt/jobs/data-science-jobs/\"\n",
    "\n",
    "# Send a GET request with a user-agent header to mimic a browser\n",
    "page = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "print(page)\n",
    "\n",
    "soup= BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f3707",
   "metadata": {},
   "source": [
    "#### Step 4.1: Extract Job Titles on Bayt\n",
    "Extract and print job titles from Bayt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be2dc97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Programme Associate - Data Analyst \n",
      "\n",
      "\n",
      "\n",
      "Odoo Developer Work In Suadi Arabia \n",
      "\n",
      "\n",
      "\n",
      "Brand Specialist, AVS (Amazon Vendor Services) \n",
      "\n",
      "\n",
      "\n",
      "Data Science Manager \n",
      "\n",
      "\n",
      "\n",
      "Senior Data Science Engineer \n",
      "\n",
      "\n",
      "\n",
      "Customer Success Manager – Focused on Data Science - 218456 \n",
      "\n",
      "\n",
      "\n",
      "Team Lead Data Scientist \n",
      "\n",
      "\n",
      "\n",
      "Quality Audits Manager \n",
      "\n",
      "\n",
      "\n",
      "Data Governance Expert \n",
      "\n",
      "\n",
      "\n",
      "Data Engineer \n",
      "\n",
      "\n",
      "\n",
      "Data Scientist \n",
      "\n",
      "\n",
      "\n",
      "Senior Data Testing Consultant - 218657 \n",
      "\n",
      "\n",
      "\n",
      "Enterprise Data Operations Assistant Analyst \n",
      "\n",
      "\n",
      "\n",
      "Area Sales Professional – Laboratory Diagnostics (Cairo & Delta Region) \n",
      "\n",
      "\n",
      "\n",
      "Senior IT Auditor \n",
      "\n",
      "\n",
      "\n",
      "Product Manager - SMS \n",
      "\n",
      "\n",
      "\n",
      "Public Relations and Communications Manager \n",
      "\n",
      "\n",
      "\n",
      "Chief Technology Officer (CTO) \n",
      "\n",
      "\n",
      "\n",
      "Chief Technology Officer (CTO) \n",
      "\n",
      "\n",
      "\n",
      "Developer, Packaging Development & Engineering \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all job titles on the page\n",
    "job_titles = soup.find_all(\"h2\", class_=\"col u-stretch t-large m0 t-nowrap-d t-trim\")\n",
    "for job_title in job_titles:\n",
    "    print(job_title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a3306",
   "metadata": {},
   "source": [
    "#### Step 4.2: Extract Job URLs on Bayt\n",
    "Extract and print the URLs for job postings on Bayt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32a9d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/en/egypt/jobs/programme-associate-data-analyst-5225967/', '/en/egypt/jobs/odoo-developer-work-in-suadi-arabia-5219562/', '/en/egypt/jobs/brand-specialist-avs-amazon-vendor-services-5218648/', '/en/egypt/jobs/data-science-manager-71400725/', '/en/egypt/jobs/senior-data-science-engineer-66105998/', '/en/egypt/jobs/customer-success-manager-focused-on-data-science-218456-72042911/', '/en/egypt/jobs/team-lead-data-scientist-72183001/', '/en/egypt/jobs/quality-audits-manager-72197103/', '/en/egypt/jobs/data-governance-expert-72106658/', '/en/egypt/jobs/data-engineer-72165704/', '/en/egypt/jobs/data-scientist-72170930/', '/en/egypt/jobs/senior-data-testing-consultant-218657-72167031/', '/en/egypt/jobs/enterprise-data-operations-assistant-analyst-72141070/', '/en/egypt/jobs/area-sales-professional-laboratory-diagnostics-cairo-delta-region-72190065/', '/en/egypt/jobs/senior-it-auditor-72190543/', '/en/egypt/jobs/product-manager-sms-72189158/', '/en/egypt/jobs/public-relations-and-communications-manager-72182180/', '/en/egypt/jobs/chief-technology-officer-cto-72179295/', '/en/egypt/jobs/chief-technology-officer-cto-72179292/', '/en/egypt/jobs/developer-packaging-development-engineering-72179405/']\n"
     ]
    }
   ],
   "source": [
    "# Extract job URLs on Bayt\n",
    "job_urls=[]\n",
    "urls= soup.find_all(\"h2\", class_=\"col u-stretch t-large m0 t-nowrap-d t-trim\")\n",
    "for job_url in urls:\n",
    "    job_urls.append(job_url.find(\"a\").attrs[\"href\"])\n",
    "print(job_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e9ff1",
   "metadata": {},
   "source": [
    "### Quizzes\n",
    "Try to find additional information from the scraped pages using the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0c1c2",
   "metadata": {},
   "source": [
    "#### Quiz 1: Find the Locations of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b407662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the location\n",
    "location = soup.find_all(\"....\", class_ =\"...\")\n",
    "for l in location:\n",
    "    print(l.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d9eec",
   "metadata": {},
   "source": [
    "#### Quiz 2: Find the Company Names of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c960a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the company\n",
    "company = soup.find_all(\"...\", class_ =\"....\")\n",
    "for c in company:\n",
    "    print(c.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693e3c4",
   "metadata": {},
   "source": [
    "#### Quiz 3: Extract Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job descreption\n",
    "desc = soup.find_all(\"....\", class_ =\"...\")\n",
    "for d in desc:\n",
    "    print(d.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
